clear all;
close all;
clc;

%Load Data Tables 
%features=importdata('features.mat');%k x m matrix with k being the number of images i.e. each row is one image
features=importdata('features2.mat');%k x m matrix with k being the number of images i.e. each row is one image
groundtruth=importdata('GroundTruth_Fuzzy.mat');

%isolate conclusive data 
gt_flags=groundtruth(:,1); 
ft=[gt_flags features]; 

%ftc = feature matrix conclusive
ftc=ft(1,:);%initialize ftc

%isolate all images that are conclusive
for i=1:size(ft,1)
    if (ft(i,1)==1)
        ftc=[ftc;ft(i,:)]; %concatenate vertically to create conclusive feature matrix
    end
end

ftc=ftc(3:size(ftc,1),2:size(ftc,2)); %strip the conclusive column and the first row

%isolate gt for conclusive data 
gtc=groundtruth(1,:);
for i=1:size(groundtruth,1);
    if (groundtruth(i,1)==1)
        gtc=[gtc;groundtruth(i,:)];
    end
end
gtc=gtc(3:size(gtc,1),2:size(gtc,2)); %strip conclusive column and the first row %note to self: the first ground truth example is rated as content-feeling

%%%%%%%%%%%%%%%%%%%%%

inputs = ftc';
targets = gtc'; 

%Cross Validation Shennanigans 
k = 6; %number of folds
samplesize = size(inputs, 2); %number of columns...
cv = cvpartition(samplesize,'kfold',k); %create a k fold cross validation 

%initialize matrices to contain the indices
trainingMat=zeros(k,cv.TrainSize(1)); %choose the first because I assume they're all the same. In this case it's 190
testingMat=zeros(k,cv.TestSize(1)); %In this case it's cv.TestSize ... 38
for i=1:k
    trainIdxs = find(training(cv,i)); %1 means in train, 0 means in test
    testIdxs = find(test(cv,i)); %1 means in test, 0 means in train
    %size(trainIdxs)
    trainingMat(i,:) = trainIdxs'; % therefore the number of the rows is the number of the fold
    testingMat(i,:) = testIdxs';
end

%[nNeurons, nLayers]=meshgrid(1:1:10, 1:1:5); %create grid of parameters (neursons per later, number of layers)
[nNeurons, nLayers]=meshgrid(10:10:50, 1:1:3);

average=zeros(numel(nNeurons),1);
for i=1:numel(nNeurons)
    nNeuronsIteration = nNeurons(i);
    nLayersIteration = nLayers(i);
    hiddenLayerSize = repmat(nNeuronsIteration,1, nLayersIteration);
    
    %Create a fitting network
    net = fitnet(hiddenLayerSize); 
    for layercount = 1:nLayersIteration
        net.layers{layercount}.transferFcn = 'radbas';
    end
    
    % Choose Input and Output Pre/Post-Processing Functions
    % For a list of all processing functions type: help nnprocess
%     net.inputs{1}.processFcns = {'removeconstantrows','mapminmax'};
%     net.outputs{2}.processFcns = {'removeconstantrows','mapminmax'};
    % For help on training function 'trainlm' type: help trainlm
    % For a list of all training functions type: help nntrain
    net.trainFcn = 'trainlm';  % Levenberg-Marquardt

    % Choose a Performance Function
    % For a list of all performance functions type: help nnperformance
    net.performFcn = 'mse';  % Mean squared error

    % Choose Plot Functions
    % For a list of all plot functions type: help nnplot
    net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
      'plotregression', 'plotfit'};

        
    %Create Neural Net for each possible training set in the k-fold
        %perf=zeros(k,1); %initialize matrix
        performance=zeros(k,1);
        for m=1:k %train the neural net for each training set in the k-fold %default training method is trainscg (which allows for training with backpropagation)
           
            % Train the Network
            [net,tr]=train(net,inputs(:,trainingMat(m,:)),targets(:,trainingMat(m,:)));
            
            outputs= net(inputs(:,testingMat(m,:)));
            errors = gsubtract(targets(testingMat(m,:)),outputs);
            
            %save output and error data 
            outputsName=strcat('outputs','-parameter-',int2str(i),'-kfold-',int2str(m),'.mat');
            save(outputsName, 'outputs');
            errorsName=strcat('errors','-parameter-',int2str(i),'-kfold-',int2str(m),'.mat');
            save(errorsName, 'errors');
            
            %performance(m,1) = mse(net,targets(:,testingMat(m,:)),outputs);
            performance(m,1) = perform(net,targets(:,testingMat(m,:)),outputs); %this is the same as mse above
            
            %save performance data
            perfName=strcat('performance','-parameter-', int2str(i), '-kfold-',int2str(m), '.mat');
            save(perfName, 'performance');
            
            name=strcat(int2str(i),'Net', int2str(m), '.mat');
            save(name, 'net')
        end
        average(i,1)=mean(performance(m,1));
        
        
end
 
[~,ind_bestnet]=min(average); %find index of the minimum value of the average mean square errors 

% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by NFTOOL
% Created Sun Apr 20 19:04:26 EDT 2014
%
% This script assumes these variables are defined:
%
%   engineInputs - input data.
%   engineTargets - target data.

%inputs = engineInputs;
%targets = engineTargets;

% Create a Fitting Network
% hiddenLayerSize = 10;
% net = fitnet(hiddenLayerSize);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
% net.inputs{1}.processFcns = {'removeconstantrows','mapminmax'};
% net.outputs{2}.processFcns = {'removeconstantrows','mapminmax'};
% 
% 
% % Setup Division of Data for Training, Validation, Testing
% % For a list of all data division functions type: help nndivide
% % net.divideFcn = 'dividerand';  % Divide data randomly
% % net.divideMode = 'sample';  % Divide up every sample
% % net.divideParam.trainRatio = 70/100;
% % net.divideParam.valRatio = 15/100;
% % net.divideParam.testRatio = 15/100;
% 
% % For help on training function 'trainlm' type: help trainlm
% % For a list of all training functions type: help nntrain
% net.trainFcn = 'trainlm';  % Levenberg-Marquardt
% 
% % Choose a Performance Function
% % For a list of all performance functions type: help nnperformance
% net.performFcn = 'mse';  % Mean squared error
% 
% % Choose Plot Functions
% % For a list of all plot functions type: help nnplot
% net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
%   'plotregression', 'plotfit'};


% % Train the Network
% [net,tr] = train(net,inputs,targets);

% % Test the Network
% % outputs = net(inputs);
% % errors = gsubtract(targets,outputs);
% % performance = perform(net,targets,outputs)

% Recalculate Training, Validation and Test Performance
% trainTargets = targets .* tr.trainMask{1};
% %valTargets = targets  .* tr.valMask{1};
% testTargets = targets  .* tr.testMask{1};
% trainPerformance = perform(net,trainTargets,outputs)
% %valPerformance = perform(net,valTargets,outputs)
% testPerformance = perform(net,testTargets,outputs)

% % View the Network
% view(net)

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, plotfit(net,inputs,targets)
%figure, plotregression(targets,outputs)
%figure, ploterrhist(errors)
